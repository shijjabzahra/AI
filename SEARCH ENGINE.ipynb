{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbt8Rr0FM1sFypAZK2SuDL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"flv3vtaCPyrH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740761167692,"user_tz":-300,"elapsed":394,"user":{"displayName":"Syeda Hijjab Zahra","userId":"01232506998458755700"}},"outputId":"8da89ae0-d627-4965-bb6b-beacdf29c7c0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Similarity with Document 1: 0.7752\n","Similarity with Document 2: 0.6350\n","Similarity with Document 3: 0.2134\n"]}],"source":["import nltk\n","nltk.download('punkt_tab') # Download the 'punkt_tab' data for sentence tokenization\n","\n","import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from math import log, sqrt\n","\n","# Sample documents\n","documents = [\n","    \"cat dog mouse\",\n","    \"dog tiger cat\",\n","    \"mouse elephant dog\"\n","]\n","\n","# Query\n","query = \"cat dog\"\n","\n","# Tokenize documents\n","doc_tokens = [word_tokenize(doc.lower()) for doc in documents]\n","query_tokens = word_tokenize(query.lower())\n","\n","# Build vocabulary\n","vocab = set(word for doc in doc_tokens for word in doc)\n","\n","# Compute TF\n","def compute_tf(tokens):\n","    tf = Counter(tokens)\n","    return {word: 1 + log(count) if count > 0 else 0 for word, count in tf.items()}\n","\n","doc_tf = [compute_tf(doc) for doc in doc_tokens]\n","query_tf = compute_tf(query_tokens)\n","\n","# Compute IDF\n","N = len(documents)\n","idf = {}\n","for word in vocab:\n","    df = sum(1 for doc in doc_tokens if word in doc)\n","    idf[word] = 1 + log(N / df) if df > 0 else 0\n","\n","# Compute TF-IDF\n","for tf in doc_tf:\n","    for word in tf:\n","        tf[word] *= idf[word]\n","for word in query_tf:\n","    query_tf[word] *= idf[word]\n","\n","# Convert to vectors\n","def to_vector(tf_dict):\n","    return np.array([tf_dict.get(word, 0) for word in vocab])\n","\n","doc_vectors = [to_vector(tf) for tf in doc_tf]\n","query_vector = to_vector(query_tf)\n","\n","# Compute Cosine Similarity\n","def cosine_similarity(vec1, vec2):\n","    dot_product = np.dot(vec1, vec2)\n","    norm1 = sqrt(np.dot(vec1, vec1))\n","    norm2 = sqrt(np.dot(vec2, vec2))\n","    return dot_product / (norm1 * norm2) if norm1 and norm2 else 0\n","\n","# Compute similarities\n","similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_vectors]\n","\n","# Display results\n","for i, score in enumerate(similarities):\n","    print(f\"Similarity with Document {i+1}: {score:.4f}\")"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load dataset\n","dataset_file = \"/content/stories - Sheet1.csv\"\n","df = pd.read_csv(dataset_file)\n","\n","# Display column names\n","print(\"Column Names:\", df.columns.tolist())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HiDoFcxe2S7","executionInfo":{"status":"ok","timestamp":1740761691090,"user_tz":-300,"elapsed":86,"user":{"displayName":"Syeda Hijjab Zahra","userId":"01232506998458755700"}},"outputId":"181ea349-15ec-48b7-ce12-79cf78e52d66"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Column Names: ['Title', 'Story-text', 'Moral']\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import nltk\n","import cv2\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from math import log, sqrt\n","\n","# Load dataset from a CSV file\n","def load_documents(filename, column_name):\n","    df = pd.read_csv(filename)\n","    return df[column_name].dropna().astype(str).tolist()\n","\n","# Sample dataset file (replace with your actual dataset file path)\n","dataset_file = \"/content/stories - Sheet1.csv\"\n","documents = load_documents(dataset_file, \"Story-text\")  # Updated with actual column name\n","\n","# Query\n","query = \"leo\"\n","\n","# Tokenize documents and query\n","doc_tokens = [word_tokenize(doc.lower()) for doc in documents]\n","query_tokens = word_tokenize(query.lower())\n","\n","# Build vocabulary including words from both documents and query\n","vocab = set(word for doc in doc_tokens for word in doc)\n","vocab.update(query_tokens) # Add query tokens to vocabulary\n","\n","# Compute TF\n","def compute_tf(tokens):\n","    tf = Counter(tokens)\n","    return {word: 1 + log(count) if count > 0 else 0 for word, count in tf.items()}\n","\n","doc_tf = [compute_tf(doc) for doc in doc_tokens]\n","query_tf = compute_tf(query_tokens)\n","\n","# Compute IDF\n","N = len(documents)\n","idf = {}\n","for word in vocab:\n","    df = sum(1 for doc in doc_tokens if word in doc)\n","    idf[word] = 1 + log(N / df) if df > 0 else 0\n","\n","# Compute TF-IDF\n","for tf in doc_tf:\n","    for word in tf:\n","        tf[word] *= idf[word]\n","for word in query_tf:\n","    query_tf[word] *= idf[word]\n","\n","# Convert to vectors\n","def to_vector(tf_dict):\n","    return np.array([tf_dict.get(word, 0) for word in vocab])\n","\n","doc_vectors = [to_vector(tf) for tf in doc_tf]\n","query_vector = to_vector(query_tf)\n","\n","# Compute Cosine Similarity\n","def cosine_similarity(vec1, vec2):\n","    dot_product = np.dot(vec1, vec2)\n","    norm1 = sqrt(np.dot(vec1, vec1))\n","    norm2 = sqrt(np.dot(vec2, vec2))\n","    return dot_product / (norm1 * norm2) if norm1 and norm2 else 0\n","\n","# Compute similarities\n","similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_vectors]\n","\n","# Display results\n","for i, score in enumerate(similarities):\n","    print(f\"Similarity with Document {i+1}: {score:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tczj97YXe-2b","executionInfo":{"status":"ok","timestamp":1740761938613,"user_tz":-300,"elapsed":251,"user":{"displayName":"Syeda Hijjab Zahra","userId":"01232506998458755700"}},"outputId":"79cb201f-94de-4807-db28-fd4d8240009c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarity with Document 1: 0.0000\n","Similarity with Document 2: 0.0000\n","Similarity with Document 3: 0.0000\n","Similarity with Document 4: 0.0000\n","Similarity with Document 5: 0.0000\n","Similarity with Document 6: 0.0000\n","Similarity with Document 7: 0.0000\n","Similarity with Document 8: 0.0000\n","Similarity with Document 9: 0.0000\n","Similarity with Document 10: 0.0000\n","Similarity with Document 11: 0.0000\n","Similarity with Document 12: 0.0000\n","Similarity with Document 13: 0.0000\n","Similarity with Document 14: 0.0000\n","Similarity with Document 15: 0.0000\n","Similarity with Document 16: 0.0000\n","Similarity with Document 17: 0.0000\n","Similarity with Document 18: 0.0000\n","Similarity with Document 19: 0.0000\n","Similarity with Document 20: 0.0000\n","Similarity with Document 21: 0.0000\n","Similarity with Document 22: 0.0000\n","Similarity with Document 23: 0.0000\n","Similarity with Document 24: 0.0000\n","Similarity with Document 25: 0.0000\n","Similarity with Document 26: 0.0000\n","Similarity with Document 27: 0.0000\n","Similarity with Document 28: 0.0000\n","Similarity with Document 29: 0.2287\n","Similarity with Document 30: 0.0000\n","Similarity with Document 31: 0.0000\n","Similarity with Document 32: 0.0000\n","Similarity with Document 33: 0.0000\n","Similarity with Document 34: 0.0000\n","Similarity with Document 35: 0.0000\n","Similarity with Document 36: 0.0000\n","Similarity with Document 37: 0.0000\n","Similarity with Document 38: 0.0000\n","Similarity with Document 39: 0.0000\n","Similarity with Document 40: 0.2337\n"]}]}]}